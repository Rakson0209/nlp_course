{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdj1XLuceOk-"
      },
      "source": [
        "# Week 03: Word Representation\n",
        "The assignment this week is to distinguish between good and bad phrases of the word \"**earn**\" (e.g., earn money). You will practice using word2vector,  one of the methods learned today, in the process. \n",
        "\n",
        "Data used in this assignment:  \n",
        "https://drive.google.com/drive/folders/1qTIrefo4EFbsVF3LXhKbiahbIrvCLUBJ?usp=sharing\n",
        "\n",
        "* train.tsv: Some phrases with labels to train and validate the classification model. There are only two types of label: 1 means *good*; 0 means *bad*.\n",
        "* test.tsv: Same format as train.tsv. It's used to test your model.\n",
        "* GoogleNews-vectors-negative300.bin.gz: a pre-trained word2vector model trained by Google ([source](https://code.google.com/archive/p/word2vec/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GzvI76xeOlH"
      },
      "source": [
        "## Requirements\n",
        "* pandas\n",
        "* tensorflow\n",
        "* sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk5Xag5ueOlI"
      },
      "source": [
        "## Read Data\n",
        "We use dataframe to store data here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UYsjz2eCeOlI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 phrase  class\n",
            "0              earn extra money and pay      1\n",
            "1       earn commissions for sales that      1\n",
            "2            hour requirement to earn a      0\n",
            "3                  earn 30 days of paid      0\n",
            "4  computer skills earn him recognition      0\n",
            "                   phrase  class\n",
            "0  degree earn 62 percent      0\n",
            "1     earn maybe 30 or 50      0\n",
            "2  earn the kind of money      1\n",
            "3      earn his 14th save      1\n",
            "4   earn a smaller amount      1\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "def loadData(path):\n",
        "    ngram = []\n",
        "    _class = []\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.strip(\"\\n\").split(\"\\t\")\n",
        "            ngram.append(line[0])\n",
        "            _class.append(int(line[1]))\n",
        "    return pd.DataFrame({\"phrase\":ngram,\"class\":_class})\n",
        "train = loadData(\"data/train.tsv\")\n",
        "print(train.head())\n",
        "test = loadData(\"data/test.tsv\")\n",
        "print(test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl8pGQx2eOlL"
      },
      "source": [
        "## load word2vec model\n",
        "<font color=\"red\">**[ TODO ]**</font> Please load [GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g) model and check the embedding of the word `language`.\n",
        "\n",
        "* package `gensim` is a good choice (Look up the documentation [here](https://radimrehurek.com/gensim/models/word2vec.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DQjCDqZyeOlO"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "w2v_model = KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "#### print \"language\" embedding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\love4\\AppData\\Local\\Temp\\ipykernel_7476\\3089305956.py:1: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  type(w2v_model.word_vec('language'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 326,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(w2v_model.word_vec('language'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fap51QcAeOlO"
      },
      "source": [
        "<font color=\"green\">Expected output: </font>\n",
        "\n",
        ">  <font face='monospace' size=3>\\[&nbsp;2.30712891e-02&nbsp;&nbsp;1.68457031e-02&nbsp;&nbsp;1.54296875e-01&nbsp; 1.27929688e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.67578125e-01&nbsp;&nbsp;3.51562500e-02&nbsp;&nbsp;1.19140625e-01&nbsp; 2.48046875e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-7.95898438e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-1.43554688e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-3.04687500e-01&nbsp;&nbsp;3.46679688e-02&nbsp;-1.85546875e-02&nbsp; 1.06933594e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.52343750e-01&nbsp;&nbsp;2.89062500e-01&nbsp;&nbsp;2.35595703e-02&nbsp;-3.80859375e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.09863281e-01&nbsp;&nbsp;4.41406250e-01&nbsp;&nbsp;3.75976562e-02&nbsp;-1.22680664e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.62353516e-02&nbsp;-2.24609375e-01&nbsp;&nbsp;7.61718750e-02&nbsp;-3.12500000e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.16064453e-02&nbsp;&nbsp;1.49414062e-01&nbsp;-4.02832031e-02&nbsp;-4.46777344e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.72851562e-01&nbsp;&nbsp;3.32031250e-02&nbsp;&nbsp;1.50390625e-01&nbsp;-5.05371094e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;2.72216797e-02&nbsp;&nbsp;3.00781250e-01&nbsp;-1.33789062e-01&nbsp;-7.56835938e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.93359375e-01&nbsp;-1.98242188e-01&nbsp;-1.27563477e-02&nbsp; 4.19921875e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.19726562e-01&nbsp;&nbsp;1.44531250e-01&nbsp;-3.93066406e-02&nbsp; 1.94335938e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-3.12500000e-01&nbsp;&nbsp;1.84570312e-01&nbsp;&nbsp;1.48773193e-04&nbsp;-1.67968750e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-7.37304688e-02&nbsp;-3.12500000e-02&nbsp;&nbsp;1.57226562e-01&nbsp; 3.30078125e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.42578125e-01&nbsp;-3.16406250e-01&nbsp;-7.32421875e-02&nbsp;-5.76171875e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.02050781e-01&nbsp;-1.08886719e-01&nbsp;&nbsp;1.24023438e-01&nbsp;-2.50244141e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.49023438e-01&nbsp;&nbsp;1.25976562e-01&nbsp;-1.79687500e-01&nbsp; 3.32031250e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;7.14111328e-03&nbsp;&nbsp;2.51953125e-01&nbsp;&nbsp;4.34570312e-02&nbsp;-4.34570312e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-3.90625000e-01&nbsp;&nbsp;1.76757812e-01&nbsp;-1.13525391e-02&nbsp;-1.97753906e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;2.79296875e-01&nbsp;&nbsp;2.36328125e-01&nbsp;&nbsp;1.19140625e-01&nbsp; 5.59082031e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.73828125e-01&nbsp;-1.10839844e-01&nbsp;-4.95605469e-02&nbsp; 2.13867188e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;6.17675781e-02&nbsp;&nbsp;1.38671875e-01&nbsp;-4.45556641e-03&nbsp; 2.55859375e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.80664062e-01&nbsp;&nbsp;5.88378906e-02&nbsp;-6.59179688e-02&nbsp;-2.08007812e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.19140625e-01&nbsp;-1.57226562e-01&nbsp;&nbsp;5.02929688e-02&nbsp;-6.29882812e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;5.00488281e-02&nbsp;-7.27539062e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-3.56445312e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.93359375e-01&nbsp;&nbsp;3.93066406e-02&nbsp;-3.36914062e-02&nbsp;-1.07421875e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;5.78613281e-02&nbsp;-8.20312500e-02&nbsp;&nbsp;1.74560547e-02&nbsp;-1.65039062e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.46484375e-01&nbsp;-3.08837891e-02&nbsp;-3.86718750e-01&nbsp; 2.49023438e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;8.74023438e-02&nbsp;-2.15820312e-01&nbsp;-4.10156250e-02&nbsp; 1.60156250e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.85546875e-01&nbsp;-2.27050781e-02&nbsp;-3.73535156e-02&nbsp; 7.86132812e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.46484375e-01&nbsp;&nbsp;6.78710938e-02&nbsp;&nbsp;1.26953125e-01&nbsp; 3.30078125e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.11328125e-01&nbsp;&nbsp;9.27734375e-02&nbsp;-3.45703125e-01&nbsp;-1.41601562e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-5.29785156e-02&nbsp;-1.50390625e-01&nbsp;-7.81250000e-02&nbsp;-1.27929688e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-4.02343750e-01&nbsp;-1.41601562e-01&nbsp;&nbsp;8.44726562e-02&nbsp; 1.08398438e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-4.44335938e-02&nbsp;&nbsp;3.73535156e-02&nbsp;&nbsp;5.61523438e-02&nbsp;-1.91406250e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.54296875e-01&nbsp;-5.12695312e-02&nbsp;-6.49414062e-02&nbsp;-8.30078125e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;7.17773438e-02&nbsp;-1.33789062e-01&nbsp;&nbsp;1.05468750e-01&nbsp; 3.33984375e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.08398438e-01&nbsp;&nbsp;1.91650391e-02&nbsp;&nbsp;2.14843750e-01&nbsp; 2.15820312e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.05468750e-01&nbsp;-1.44531250e-01&nbsp;&nbsp;4.32128906e-02&nbsp;-2.71484375e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-3.78906250e-01&nbsp;&nbsp;1.09863281e-01&nbsp;-8.15429688e-02&nbsp;-6.12792969e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.33789062e-01&nbsp;&nbsp;9.71679688e-02&nbsp;-1.04370117e-02&nbsp;-1.21093750e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.44140625e-01&nbsp;&nbsp;1.02050781e-01&nbsp;&nbsp;1.10839844e-01&nbsp;-1.00585938e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.71875000e-01&nbsp;-3.61328125e-02&nbsp;-4.39453125e-02&nbsp; 2.83203125e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-8.93554688e-02&nbsp;-1.70898438e-01&nbsp;&nbsp;2.46093750e-01&nbsp; 1.16699219e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;8.39843750e-02&nbsp;-1.32812500e-01&nbsp;-1.61132812e-01&nbsp;-1.39648438e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-8.59375000e-02&nbsp;-1.37695312e-01&nbsp;-9.32617188e-02&nbsp;-1.33789062e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.65039062e-01&nbsp;&nbsp;4.93164062e-02&nbsp;-1.21093750e-01&nbsp;-2.11914062e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;1.61132812e-01&nbsp;-1.07421875e-01&nbsp;-3.97949219e-02&nbsp;-3.51562500e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-5.02929688e-02&nbsp;&nbsp;1.46484375e-01&nbsp;-4.68750000e-02&nbsp; 4.17480469e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.27929688e-01&nbsp;-9.76562500e-02&nbsp;-2.46093750e-01&nbsp; 6.78710938e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.30468750e-01&nbsp;&nbsp;1.80664062e-02&nbsp;&nbsp;3.54003906e-02&nbsp; 7.32421875e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.23632812e-01&nbsp;-1.25976562e-01&nbsp;&nbsp;2.12890625e-01&nbsp;-3.93066406e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.41699219e-02&nbsp;-9.61914062e-02&nbsp;&nbsp;7.51953125e-02&nbsp;-1.46484375e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;-8.83789062e-02&nbsp;-4.88281250e-02&nbsp; 2.32421875e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;3.30078125e-01&nbsp;&nbsp;1.59179688e-01&nbsp;-2.35351562e-01&nbsp;-1.25976562e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;2.68554688e-02&nbsp;-5.29785156e-02&nbsp;-6.59179688e-02&nbsp;-2.17773438e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-6.37817383e-03&nbsp;-2.53906250e-01&nbsp;&nbsp;2.28515625e-01&nbsp; 4.93164062e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;3.54003906e-02&nbsp;&nbsp;1.66992188e-01&nbsp;-7.27539062e-02&nbsp;-2.53906250e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.34765625e-01&nbsp;&nbsp;3.69140625e-01&nbsp;&nbsp;1.83593750e-01&nbsp;-1.64062500e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;2.26562500e-01&nbsp;-8.88671875e-02&nbsp;&nbsp;3.69140625e-01&nbsp; 5.54199219e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-3.63769531e-02&nbsp;-1.48437500e-01&nbsp;&nbsp;9.13085938e-02&nbsp; 2.47955322e-04<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;2.67578125e-01&nbsp;-1.63085938e-01&nbsp;&nbsp;1.19628906e-01&nbsp; 2.77343750e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.49414062e-01&nbsp;&nbsp;1.33789062e-01&nbsp;-8.25195312e-02&nbsp;-1.74804688e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.77734375e-01&nbsp;&nbsp;2.06054688e-01&nbsp;&nbsp;5.07812500e-02&nbsp;-2.08007812e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.74804688e-01&nbsp;&nbsp;9.66796875e-02&nbsp;&nbsp;6.98242188e-02&nbsp;-5.79833984e-04<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;9.22851562e-02&nbsp;&nbsp;7.95898438e-02&nbsp;&nbsp;1.41601562e-01&nbsp; 8.72802734e-03<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-8.05664062e-02&nbsp;&nbsp;4.80957031e-02&nbsp;&nbsp;2.49023438e-01&nbsp;-1.64062500e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-4.66308594e-02&nbsp;-2.81250000e-01&nbsp;-1.66015625e-01&nbsp;-2.22656250e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-2.32421875e-01&nbsp;&nbsp;1.32812500e-01&nbsp;&nbsp;4.15039062e-02&nbsp; 1.15234375e-01<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-7.66601562e-02&nbsp;-1.10839844e-01&nbsp;-1.97265625e-01&nbsp; 3.06396484e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.03515625e-01&nbsp;&nbsp;2.49023438e-02&nbsp;-2.52685547e-02&nbsp; 3.39355469e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;4.29687500e-02&nbsp;-1.44531250e-01&nbsp;&nbsp;2.12402344e-02&nbsp; 2.28271484e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.88476562e-01&nbsp;&nbsp;3.22265625e-01&nbsp;-1.13281250e-01&nbsp;-7.61718750e-02<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;&nbsp;2.94921875e-01&nbsp;-1.33789062e-01&nbsp;-1.80664062e-02&nbsp;-6.25610352e-03<br> </font>\n",
        ">  <font face='monospace' size=3>&nbsp;-1.62353516e-02&nbsp;&nbsp;5.98144531e-02&nbsp;&nbsp;1.21582031e-01&nbsp; 4.17480469e-02\\] </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL4Gqhyw56oX"
      },
      "source": [
        "<font color=\"red\">**[ TODO ]**</font> You can also find the top-N most similar words. Try it! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zq-Jwhxe5jDy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('elementary', 0.7868632078170776),\n",
              " ('schools', 0.7411909103393555),\n",
              " ('shool', 0.6692329049110413),\n",
              " ('elementary_schools', 0.6597153544425964),\n",
              " ('kindergarten', 0.6529810428619385)]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#### print top 5 most similar words to \"school\"\n",
        "w2v_model.most_similar('school', topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUUOFU4J4Anl"
      },
      "source": [
        "<font color=\"green\">Expected output: </font>\n",
        ">  <font face='monospace' size=3>\n",
        "[('elementary', 0.7868632078170776),<br>\n",
        "&nbsp;('schools', 0.7411909103393555),<br>\n",
        "&nbsp;('shool', 0.6692329049110413),<br>\n",
        "&nbsp;('elementary_schools', 0.6597153544425964),<br>\n",
        "&nbsp;('kindergarten', 0.6529811024665833)]</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIk5hWfGeOlR"
      },
      "source": [
        "## Preprocessing\n",
        "Preprocess the two tsv files here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUKN7pEKeOlS"
      },
      "source": [
        "#### adjust the ratio of the two classes of training data\n",
        "In the training data, the ratio of good phrases to bad phrases is about one to thirty. That will make training classification unsatisfactory, so we need to adjust the ratio. Reducing bad phrases and adding good phrases are both common way.\n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Please adjust the ratio of good phrases to bad phrases however you think is best and output the number of the two classes for demo.\n",
        "\n",
        "You need to explain why you chose this ratio and how you did it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "dic = {'phrase':[], 'label':[]}\n",
        "df = pd.DataFrame(columns=['phrase', 'label'])\n",
        "import os\n",
        "file_path = os.path.join('data','train_orig.tsv')\n",
        "with open(file_path, 'r', encoding='UTF-8') as f:\n",
        "    for line in f:\n",
        "        substr = line[:-1].split(sep='\\t')\n",
        "        dic['phrase'].append(substr[0])\n",
        "        dic['label'].append(int(substr[1]))\n",
        "df = pd.DataFrame.from_dict(dic)\n",
        "#### print the number of training data of two classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_bad = df.loc[df['label'] == 0]\n",
        "good = df.loc[df['label'] == 1]\n",
        "bad = df_bad.sample(n=6105)\n",
        "df = pd.concat([good, bad])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "193493\n",
            "6105\n"
          ]
        }
      ],
      "source": [
        "print((len(df_bad)+len(good)) == len(dic['phrase']))\n",
        "print(len(df_bad))\n",
        "print(len(good))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = df.sample(frac=1)\n",
        "test = pd.read_csv('data/test.tsv',  delimiter='\\t', names=['phrase', 'label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V40xvY1eOlT"
      },
      "source": [
        "#### number words\n",
        "Give each word a unique number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "N2g3NLJ9eOlT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok = Tokenizer()\n",
        "tok.fit_on_texts(pd.concat([train,test],ignore_index=True)['phrase'].tolist())\n",
        "vocab_size = len(tok.word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj0V1G_AeOlT"
      },
      "source": [
        "#### convert phrases into numbers\n",
        "Your model can't understand words, so we have to do this transform first. \n",
        "\n",
        "The number should be the same as the last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "FdwSF-1JeOlU"
      },
      "outputs": [],
      "source": [
        "train_encoded_phrase = tok.texts_to_sequences(train['phrase'])\n",
        "test_encoded_phrase = tok.texts_to_sequences(test['phrase'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hhTKdm7eOlV"
      },
      "source": [
        "#### <font color=\"red\">**[ TODO ]**</font> padding\n",
        "Make all phrases the same length. The longest phrases in the two tsv files have five tokens. Hence, we should add zeroes to all the phrases that are shorter than five. \n",
        "- we suggest using `pad_sequences`, but you can do it however you like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "LF8rQwmneOlV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  0   0   1  52 261]\n",
            " [  0   1 754 198   2]\n",
            " [  8   1  29 121  18]\n",
            " [  0 909   2   1   4]\n",
            " [  1  79  27  49  12]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "X_train = pad_sequences(train_encoded_phrase, maxlen=5)\n",
        "X_test = pad_sequences(test_encoded_phrase, maxlen=5)\n",
        "print(X_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbE9uyk0eOlW"
      },
      "source": [
        "#### <font color=\"red\">**[ TODO ]**</font> one hot encode the labels\n",
        "- we suggest using `to_categorical`, but again, you can use whatever you like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "SJkFyC8_eOlX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(train['label'], dtype='int32')\n",
        "y_test = to_categorical(test['label'], dtype='int32')\n",
        "print(y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJnIDUSeOlX"
      },
      "source": [
        "#### split training data into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12210, 2)"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "r32haPqreOlY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.20,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKthy0kTeOlY"
      },
      "source": [
        "#### <font color=\"red\">**[ TODO ]**</font> creating the embedding matrix\n",
        "The embedding matrix is used by the classification model. It should be a list of lists. Each sub-list is an embedding vector of a word and the order of all embedding vectors should be same as the word index numbering from the *tokenizer*. The tokenizer output is stored in a dictionary. You can check it using `tok.word_index.items()`.\n",
        "\n",
        "Make the embedding matrix. Our example model will need one, but you can skip it if the classification model you're using doesn't need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_matrix = []\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\love4\\AppData\\Local\\Temp\\ipykernel_18312\\2236121493.py:3: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
            "  embedding_matrix.extend(w2v_model.word_vec(key))\n"
          ]
        }
      ],
      "source": [
        "for key, value in tok.word_index.items():\n",
        "    if key in w2v_model.key_to_index.keys():\n",
        "        embedding_matrix.extend(w2v_model.word_vec(key))\n",
        "    else:\n",
        "        embedding_matrix.extend(np.zeros(300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c9JvMZaeOlZ"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpgeqgmWeOlZ"
      },
      "source": [
        "#### build model\n",
        "<font color=\"red\">**[ TODO ]**</font> Please build your classification model by ***keras*** here. Don't worry if you don't know how, just use the one given below. Feel free to make any changes or even build your own.\n",
        "\n",
        "You **must** use the pre-trained word2vec model to represent the words of phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "XRlAGm9teOla"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\love4\\anaconda3\\envs\\tf26\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Flatten , Embedding, LSTM, LSTM, ReLU, Dropout\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.layers import ReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=300,input_length=5,embeddings_initializer=Constant(embedding_matrix)))\n",
        "model.add(LSTM(64,return_sequences=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2,activation='sigmoid')) \n",
        "model.compile(optimizer=RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "7EVyMofjeOla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (None, 5, 300)            987300    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 64)                93440     \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,080,870\n",
            "Trainable params: 1,080,870\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSG0J5P7eOla"
      },
      "source": [
        "#### train\n",
        "Train classification model here.\n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Adjust the hyperparameter to optimize the validation accuracy and validation loss.\n",
        "\n",
        "* The higher the accuracy, the better; the lower the validation, the better.\n",
        "* **number of epoch** and **batch size** are the most important\n",
        "  * Start with a smaller number of epochs first--it is directly correlated to the training time, and you don't want to spend too much time waiting!\n",
        "  * Usually the larger the batch size the better, but the batch size you are able to use depends on you computing power, so start small and increase gradually. It is recommended to use powers of 2 (2, 4, 8, 16, ...) for batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "MSs4f9ELeOlb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "4884/4884 [==============================] - 15s 3ms/step - loss: 0.0322 - accuracy: 0.9943 - val_loss: 0.1750 - val_accuracy: 0.9771\n",
            "Epoch 2/6\n",
            "4884/4884 [==============================] - 15s 3ms/step - loss: 0.0323 - accuracy: 0.9942 - val_loss: 0.1926 - val_accuracy: 0.9791\n",
            "Epoch 3/6\n",
            "4884/4884 [==============================] - 15s 3ms/step - loss: 0.0328 - accuracy: 0.9942 - val_loss: 0.1805 - val_accuracy: 0.9791\n",
            "Epoch 4/6\n",
            "4884/4884 [==============================] - 15s 3ms/step - loss: 0.0322 - accuracy: 0.9938 - val_loss: 0.1851 - val_accuracy: 0.9795\n",
            "Epoch 5/6\n",
            "4884/4884 [==============================] - 15s 3ms/step - loss: 0.0350 - accuracy: 0.9943 - val_loss: 0.1812 - val_accuracy: 0.9775\n",
            "Epoch 6/6\n",
            "4884/4884 [==============================] - 15s 3ms/step - loss: 0.0339 - accuracy: 0.9934 - val_loss: 0.1790 - val_accuracy: 0.9812\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2379ecf9490>"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train,y_train,validation_data=(X_val,y_val),batch_size= 2, epochs= 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJIEsewSeOlc"
      },
      "source": [
        "#### test\n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Test your model by test.tsv and output the accuracy. Beat the accuracy baseline: **0.98** for extra points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "nDzmeBV4eOlc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9820\n",
            "0.9819999933242798\n"
          ]
        }
      ],
      "source": [
        "accuracy = model.evaluate(X_test,y_test)\n",
        "print(accuracy[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model\\assets\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.saved_model.save(model, \"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxVkZUuFeOlc"
      },
      "source": [
        "## Show wrong prediction results\n",
        "Observing wrong prediction result may help you improve your prediction.\n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> show the wrong prediction results like this: \n",
        "\n",
        "<img src=\"https://imgur.com/BOTMyZH.jpg\" width=30%><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVqH4lvleOld"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KprbNm7KeOle"
      },
      "source": [
        "## TA's Notes\n",
        "\n",
        "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1OKbXhcv6E3FEQDPnbHEHEeHvpxv01jxugMP7WwnKqKw/edit#gid=258852025) to reserve demo time.  \n",
        "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to elearn. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
        "<br>Note that **late submission will not be allowed**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQl6lTDzeOlf"
      },
      "source": [
        "## Learning Resource\n",
        "[Deep Learning with Python](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf)\n",
        "\n",
        "[Classification on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tf26')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "cee32ab8efbe605bef27a42257d41fdd0697c4b181f3e1656737a77e1e48cbfe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
